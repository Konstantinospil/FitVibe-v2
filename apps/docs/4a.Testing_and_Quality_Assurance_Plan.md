---
title: "FitVibe — Testing & Quality Assurance Plan"
version: "v3.0"
status: "Approved (MVP → GA baseline)"
review_cadence: "Per release and quarterly"
date: "2025-10-26"
license: "MIT"
---

# 1. Purpose & Scope

This plan defines **how FitVibe verifies, validates, and continuously assures quality** across the product lifecycle, aligning with the PRD, TDD, and VDD. It is both a **policy** (what must be true to ship) and a **playbook** (how we test, measure, and gate releases).

**Scope:** Frontend SPA, Backend/API & DB schema, Infrastructure & CI/CD, Observability, Security/Privacy & Compliance, Backup/DR.
**Out of scope (MVP):** Real-time collaboration, wearables integrations, advanced notifications (covered in future increments with addenda).

---

# 2. Quality Objectives (Targets & Gates)

| Objective                      | Target / Gate                                                                                                     | Evidence                                    |
| ------------------------------ | ----------------------------------------------------------------------------------------------------------------- | ------------------------------------------- |
| Functional correctness         | 100% pass for **Register → Verify → Login → Plan → Complete → Analytics Export**                                  | Playwright E2E report + traces              |
| Test coverage                  | **≥80% lines & branches** repo-wide; **≥90%** on auth/session/points critical code                                | Coverage HTML + thresholds in CI            |
| API performance (p95 by group) | auth ≤200 ms · crud ≤300 ms · feed ≤400 ms · analytics ≤600 ms; ≤10% regression                                   |
| Cold start                     | /health returns 200 in ≤5 s from container start                                                                  |
| Request/Response sizes         | Request body ≤2 MB; Response body ≤1 MB; gzip/Brotli for >1 KB                                                    |
| Telemetry hygiene              | No PII in logs/metrics/labels (lint & sample scan)                                                                |
| Web performance                | LCP P75 < 2.5 s (mid-tier 4G device); ≤5 API calls on first paint (key screens)                                   | Lighthouse CI + Playwright network          |
| Accessibility                  | WCAG 2.1 AA, axe/Lighthouse score **≥90** (Auth, Planner, Logger, Dashboard)                                      | a11y JSON & screenshot notes                |
| Security & Supply chain        | **0 High/Critical** SCA; ZAP baseline **no High**; SBOM per build; images **signed (cosign)**; **pinned digests** | SCA/ZAP reports, SBOM, signature verify log |
| Privacy & GDPR                 | DSR export/delete **≤30 days**; delete propagates to backups **≤14 days** (evidence)                              | DSR records + DR drill report               |
| Availability                   | SLO ≥99.5% monthly (error budget ≤0.5%)                                                                           | SLO dashboard                               |
| Visual integrity               | **Tokens contract** passes; **visual diff ≤0.2%** on critical pages (light/dark · xs/sm/md/lg)                    | Visual snapshot diffs                       |

**Policy:** Any gate failure blocks PR merge and/or release. Waivers must include **ID, rationale, expiry, and approver** (PO + Tech Lead + QA Lead).

---

# 3. Governance & Roles

| Role                 | Responsibilities                                                                       |
| -------------------- | -------------------------------------------------------------------------------------- |
| **QA Lead**          | Owns this plan, curates RTM, defines CI gates, tracks flakiness, approves waivers.     |
| **Tech Lead**        | Ensures testability hooks; reviews reliability budgets; co-owns perf & security gates. |
| **Product Owner**    | Validates acceptance criteria; signs UAT; manages risk & scope trade-offs.             |
| **Design**           | Maintains VDD & tokens; reviews visual baseline updates; approves visual diffs.        |
| **Security/Privacy** | Approves security scans and GDPR evidence; audits headers/TLS/DSR artifacts.           |
| **Engineers**        | Ship features with tests; keep fixtures deterministic; fix flaky tests promptly.       |

**Review cadence:** per release (PRD delta + RTM sync) and quarterly (coverage, perf trend, escaped defects).

---

# 4. Definitions & Abbreviations

**AC** (Acceptance Criteria), **RTM** (Requirements Traceability Matrix), **DSR** (Data Subject Request), **RPO/RTO** (Recovery Point/Time Objective), **SBOM**, **LCP**, **SLO**, **WCAG**.

---

# 5. Requirements Traceability (RTM)

We maintain a living RTM at `docs/rtm_comprehensive.csv`, mapping **PRD FR/NFR → AC IDs → Test IDs/paths → Evidence artifacts**.
The **AC master** lives in `docs/qa/AC_Master.md` (reviewed with PRD updates). CI checks ensure referenced tests exist and are green.

---

# 6. Risk-Based Strategy

We prioritize test depth based on **likelihood × impact** (auth flows, session completion, feed privacy, analytics aggregation). High-risk areas receive **redundant coverage** (unit + integration + E2E + contract + security/perf where applicable).

---

# 7. Test Architecture & Determinism

- **Unit:** Jest/Vitest; fakes for clock/UUID; seeded PRNG; golden responses for some helpers.
- **Integration:** Supertest against API with **ephemeral Postgres**; migrations applied; transactional tests; object storage & mail fakes; AV fake (EICAR positive).
- **Contract:** zod ↔ OpenAPI schema parity; negative assertions (no leakage of scoring internals).
- **E2E:** Playwright across Chromium/WebKit/Firefox; stable data seeds; **FakeClock**; masked dynamic regions in snapshots.
- **Data migrations:** order/drift/index/FK/enum contracts; reproducible seeds.
- **Observability:** metrics contract tests; trace coverage spot-checks.

---

# 7.1 Test Reliability & Flake Management

## 7.1.1 Flaky Test Definition

A test is **flaky** if it produces inconsistent results (pass/fail) on the same code commit without deterministic cause.

**Trigger conditions:**

- Fails ≥1 time in 50 runs on same commit SHA
- Requires >1 retry to pass in Playwright/Jest
- Timing-dependent (race conditions, hardcoded sleeps)

## 7.1.2 Test Reliability SLI/SLO

**SLI (Service Level Indicator):**

```
Flake Rate = (Flaky Test Runs / Total Test Runs) × 100
```

**Measurement window:** Rolling 7 days

**SLO (Service Level Objective):**

- **Target:** Flake rate ≤ **2%** repo-wide
- **Critical paths (auth, sessions, checkout):** ≤ **1%**

**Tracking:**

- Playwright: `reporter: ['json', { outputFile: 'reports/test-results.json' }]`
- Parse retry counts from JSON; aggregate to dashboard

## 7.1.3 Flake Detection & Quarantine

**Automated detection (CI):**

```js
// scripts/ci/detect-flakes.mjs
import results from "../reports/test-results.json";

const flakes = results.suites
  .flatMap((s) => s.specs)
  .filter((spec) => spec.tests.some((t) => t.results.length > 1)); // Retried

if (flakes.length > 0) {
  console.warn(`⚠️  Detected ${flakes.length} flaky tests:`);
  flakes.forEach((f) => console.warn(`  - ${f.file}:${f.line} "${f.title}"`));

  // Auto-create issue if ≥3 flakes in 7 days
  // (GitHub API call with label: 'flaky-test')
}
```

**Quarantine process:**

1. **Trigger:** Test flakes 3× in 7 days
2. **Action:**
   - Auto-skip with `test.skip` + `@flaky` annotation
   - GitHub issue created with:
     - Trace links (last 3 failures)
     - Suspected root cause (timing, network, seed data)
     - Owner assignment (test file author)
3. **SLA:** Fix or delete within **1 sprint** (2 weeks)
4. **Escalation:** If unresolved after 1 sprint, test is **deleted** (not disabled)

## 7.1.4 Root Cause Categories & Mitigations

| Category            | Symptoms                          | Mitigation                                                        |
| ------------------- | --------------------------------- | ----------------------------------------------------------------- |
| **Timing**          | `waitForSelector` timeouts        | Use `waitForLoadState('networkidle')`; increase timeout sparingly |
| **Network**         | Flaky API mocks                   | MSW strict mode; assert route handlers called                     |
| **Seed data**       | Unique constraint violations      | Transactional cleanup; deterministic UUIDs                        |
| **Animation**       | Screenshot pixel diffs            | Disable animations in test; mask dynamic regions                  |
| **Race conditions** | Intermittent DB constraint errors | Use transactions; serialize writes; idempotency                   |
| **Env-specific**    | Passes locally, fails CI          | Reproduce with Docker; check font/timezone/deps                   |

## 7.1.5 Reporting & Review

**Weekly flake report (auto-generated):**

- Top 10 flakiest tests (by retry count)
- Flake rate trend (7d/30d)
- Quarantined tests status (age, owner, issue link)

**Quarterly review (QA Lead):**

- Flake rate vs SLO
- Patterns: Which suites/frameworks are flakiest?
- Action items: Refactor problematic test patterns

## 7.1.6 Playwright Retries Configuration

```ts
// playwright.config.ts
export default defineConfig({
  retries: process.env.CI ? 2 : 0, // Retry twice on CI only
  reporter: [
    ["list"],
    ["json", { outputFile: "reports/test-results.json" }],
    ["html", { open: "never" }],
  ],
});
```

**Policy:** Retries are a **crutch**, not a solution. Any test requiring >1 retry to pass must be investigated.

---

# 8. Test Levels (What & How)

## 8.1 Functional

- **Auth & Security:** email verify (TTL 15m, resend ≤3/h), lockout after 10 tries, 2FA TOTP + backup codes, refresh rotation & reuse detection, **no user enumeration**.
- **Profiles:** alias uniqueness (case-insensitive), avatar upload (≤5 MB, EXIF stripped, AV scan).
- **Exercise Library:** RBAC (admins manage global), soft delete & audit trail, search indexes (GIN/trigram).
- **Sessions:** lifecycle & cloning with attribution, RRULE DST-safe, public↔private sync to feed ≤5 min.
- **Analytics:** matview refresh on completion; CSV/JSON export; range guards (413).
- **Points & Badges:** non-negative bounded scoring; on-complete awarding; no formula exposure.
- **Sharing & Feed:** default privacy = private; discoverability controlled by flags; link token lifecycle.
- **i18n:** static token catalogs (EN/DE); fallback EN; lint prevents stray literals; manual language switch persists.

## 8.2 Non-Functional

- **Performance:** grouped k6 scenarios (auth/CRUD/feed/analytics); thresholds & regression guard; response size ≤1 MB; gzip/Brotli required; key dashboards ≤600 ms p95 on CI dataset.
- **Accessibility:** axe/Lighthouse ≥90; keyboard nav; focus ring; contrast AA; consent banner blocks non-essential analytics until opt-in.
- **Security:** SCA (npm/OSV), ZAP baseline (auth ctx), headers (CSP, HSTS, Referrer-Policy, Permissions-Policy), CSRF/CORS, secrets scan, rate-limit/lockout abuse scenarios.
- **Supply chain:** SBOM (Syft), image scan (Trivy/Grype), **cosign sign + verify**, **pinned digest** deploys.
- **Visual design:** token contract (CSS variables & `tokens.json`), visual regression snapshots (light/dark; xs/sm/md/lg); responsive overflow guards; reduced-motion parity.
- **Observability:** `/metrics` exposes `{method,route,status_code}` histograms, DB query histogram, RUM LCP gauge; trace spans for API handlers with DB child spans.

---

## 8.2.1 Performance Baseline Management

### Purpose

Prevent false positives from regression guard while allowing legitimate performance changes (new features, intentional trade-offs).

### Baseline Storage

**Location:** `tests/perf/baselines/{branch}.json`

**Structure:**

```json
{
  "version": "1.0",
  "commit": "abc123def",
  "date": "2025-10-26T10:00:00Z",
  "groups": {
    "auth": { "p50": 145, "p95": 198, "p99": 250 },
    "crud": { "p50": 210, "p95": 295, "p99": 380 },
    "feed": { "p50": 310, "p95": 395, "p99": 510 },
    "analytics": { "p50": 450, "p95": 590, "p99": 750 }
  },
  "infrastructure": {
    "runner": "GitHub Actions ubuntu-latest",
    "cpu": "2 cores",
    "memory": "7GB",
    "db": "PostgreSQL 16.2 (shared)"
  }
}
```

### Update Process

**Scenario 1: Intentional Performance Change**

1. Engineer proposes new baseline in PR description:

```markdown
## Performance Impact

- **Change:** Added session aggregation caching
- **Expected:** Analytics p95 improves 200ms → 150ms
- **Justification:** Issue #1234 (analytics too slow)
```

2. k6 runs; comparison report attached:

```
   Group      | Old p95 | New p95 | Δ      | Status
   -----------|---------|---------|--------|--------
   analytics  | 590ms   | 420ms   | -29%   | ✅ Improvement
```

3. **Approval required:** Tech Lead + PO (if regression) or Tech Lead only (if improvement)

4. **Merge:** Baseline auto-updates on merge to `main`

**Scenario 2: Infrastructure Change**

- CI runner upgrade, DB version change, etc.
- **Action:** Regenerate all baselines with annotation in commit:

```
  perf: Regenerate baselines for GitHub Actions ubuntu-22.04

  Infrastructure change: ubuntu-20.04 → 22.04
  No application code changes.
```

**Scenario 3: Fluke/Transient Degradation**

- Single PR shows regression but no code changes justify it
- **Action:** Re-run k6 3× on same commit; take median p95
- If regression persists → investigate (CI resource contention, DB query plan change)
- If regression clears → noise; no baseline update

### Automated Baseline Update (Post-Merge)

```yaml
# .github/workflows/update-perf-baseline.yml
name: Update Performance Baseline
on:
  push:
    branches: [main]
    paths: ["apps/**", "packages/**"] # Only on app changes

jobs:
  update_baseline:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: grafana/setup-k6@v1
      - run: k6 run tests/perf/k6/budgets.js --out json=reports/perf-new.json
      - name: Update baseline (if 3 consecutive greens)
        run: node scripts/perf/update-baseline.mjs reports/perf-new.json
        # Logic: Only update if last 3 main runs were green
      - uses: stefanzweifel/git-auto-commit-action@v5
        with:
          commit_message: "perf: Auto-update baseline [skip ci]"
          file_pattern: tests/perf/baselines/main.json
```

### Regression Guard Logic

```js
// tests/perf/k6/assert-budgets.mjs
const baseline = JSON.parse(fs.readFileSync("tests/perf/baselines/main.json"));
const current = parseK6Results("reports/perf.json");

for (const group of ["auth", "crud", "feed", "analytics"]) {
  const oldP95 = baseline.groups[group].p95;
  const newP95 = current.groups[group].p95;
  const delta = ((newP95 - oldP95) / oldP95) * 100;

  if (delta > 10) {
    // 10% regression threshold
    console.error(`❌ ${group} p95 regressed ${delta.toFixed(1)}% (${oldP95}ms → ${newP95}ms)`);
    process.exit(1);
  }

  console.log(`✅ ${group} p95: ${newP95}ms (Δ ${delta > 0 ? "+" : ""}${delta.toFixed(1)}%)`);
}
```

### Normalization (CI Runner Variance)

**Problem:** GitHub Actions runners vary in performance (~15% variance)

**Solution:**

1. Run k6 **3 times** per CI job
2. Discard highest and lowest p95
3. Use **median** p95 for comparison

```js
const runs = [runK6(), runK6(), runK6()];
const p95s = runs.map((r) => r.groups.auth.p95).sort((a, b) => a - b);
const medianP95 = p95s[1]; // Middle value
```

### Quarterly Review

**QA Lead + Tech Lead + DevOps:**

- Compare baseline drift over time
- Correlate with infrastructure changes, traffic growth
- Adjust budgets if SLO targets shift (e.g., analytics now <500ms instead of <600ms)

# 9. Test Environments & Data

| Env                | Purpose                 | Data                    | Branch/Gate     |
| ------------------ | ----------------------- | ----------------------- | --------------- |
| **Local**          | Dev loop                | Synthetic seeds & fakes | none            |
| **CI (ephemeral)** | Automated checks        | Migrated DB + seeds     | PR gate         |
| **Staging**        | UAT, perf/a11y/security | Anonymized/synthetic    | main protected  |
| **Production**     | Live                    | Real                    | tag + approvals |

**Data policy:** Non-PII in CI; scoped SAs; ephemerals destroyed post-run.

---

## 9.1 Test Data Strategy

### 9.1.1 Synthetic Data Generation

**Framework:** Faker.js with deterministic seeds per test suite

**Factory pattern (`tests/fixtures/factories/`):**

```ts// user.factory.ts
import { faker } from '@faker-js/faker';export function createUser(seed: number, overrides = {}) {
faker.seed(seed);
return {
id: faker.string.uuid(),
email: faker.internet.email(),
alias: faker.internet.userName(),
dateOfBirth: '1990-01-01',
gender: 'prefer_not_to_say',
role: 'user',
createdAt: faker.date.past(),
...overrides
};
}// session.factory.ts
export function createSession(userId: string, seed: number, overrides = {}) {
faker.seed(seed);
return {
id: faker.string.uuid(),
userId,
title: faker.company.catchPhrase(),
scheduledAt: faker.date.future(),
status: 'planned',
privacy: 'private',
...overrides
};
}
```

### 9.1.2 Referential Integrity

**Constraint:** All foreign keys must reference existing parent records

**Approach:**

1. Generate parent entities first (users, exercises)
2. Child factories accept parent IDs as required params
3. Transactional test setup ensures cleanup

**Example test setup:**

```tsdescribe('Session completion', () => {
let user, exercise, session;beforeEach(async () => {
user = await db.users.create(createUser(12345));
exercise = await db.exercises.create(createExercise(67890));
session = await db.sessions.create(createSession(user.id, 11111, {
exercises: [{ exerciseId: exercise.id, sets: 3 }]
}));
});afterEach(async () => {
await db.$transaction([
db.sessions.deleteMany(),
db.exercises.deleteMany(),
db.users.deleteMany()
]);
});
});
```

### 9.1.3 Seed Versioning & Lineage

**Storage:** `tests/fixtures/seeds/v{major}.{minor}/`

**Structure:**tests/fixtures/seeds/
├─ v1.0/
│ ├─ users.json # 10 synthetic users
│ ├─ exercises.json # 50 exercises (5 global admin, 45 user-owned)
│ ├─ sessions.json # 100 sessions (varied statuses)
│ └─ manifest.json # checksums, counts, relationships
├─ v1.1/ # Added points/badges data
│ ├─ users.json
│ ├─ exercises.json
│ ├─ sessions.json
│ ├─ points.json # NEW
│ └─ manifest.json
└─ current -> v1.1/ # Symlink to active version

**Versioning policy:**

- **Patch (x.y.Z):** Quantity changes only (more users/sessions)
- **Minor (x.Y.0):** New entity types added (points, badges)
- **Major (X.0.0):** Breaking schema changes (column removals)

**CI usage:**
`bashpnpm db:seed:load tests/fixtures/seeds/current/`

### 9.1.4 Anonymization (Staging)

**When:** Staging requires prod-like scale for perf testing

**Process:**

1. Export production schema + row counts
2. Apply anonymization transforms:

````sqlUPDATE users SET
email = 'user_' || id || '@example.test',
alias = 'user_' || id,
avatar_url = NULL,
phone = NULL;UPDATE sessions SET
notes = CASE
WHEN notes IS NOT NULL THEN 'Anonymized training notes'
ELSE NULL
END;```
3. Validate: no real emails/phones remain (regex scan)
4. Export to staging dump

**Frequency:** Monthly or on-demand for perf testing

### 9.1.5 Refresh Cadence & Drift Detection

**Quarterly review (QA Lead + BE Lead):**
1. Compare seed schema vs current DB schema
2. Identify missing columns, new constraints, enum changes
3. Regenerate seeds if drift detected

**Automated drift detection (CI):**
```ts// tests/db/seed-drift.test.ts
import { db } from './client';
import manifest from '../fixtures/seeds/current/manifest.json';test('Seed data matches current schema', async () => {
const tableInfo = await db.$queryRaw    SELECT column_name, data_type, is_nullable     FROM information_schema.columns     WHERE table_name = 'users'  ;const seedSchema = manifest.tables.users.columns;
expect(tableInfo).toMatchObject(seedSchema); // Deep equality
});```

**Action on drift:** Fail CI with message: "Seed data outdated. Run `pnpm db:seed:regenerate`"

### 9.1.6 Environment Parity Matrix

| Component        | Local | CI    | Staging | Prod  |
|-----------------|-------|-------|---------|-------|
| DB Version      | 16.2  | 16.2  | 16.2    | 16.2  |
| Node Version    | 20    | 20    | 20      | 20    |
| Seed Version    | v1.1  | v1.1  | Anon    | Real  |
| Feature Flags   | All   | Prod  | Prod    | Prod  |
| Observability   | Mock  | Real  | Real    | Real  |
| Object Storage  | Fake  | Fake  | Real    | Real  |
| Email/SMS       | Fake  | Fake  | Sandbox | Prod  |

**Gate:** CI fails if local/CI parity breaks (Node version mismatch, etc.)
---

# 10. Tooling

- **Unit/Integration:** Jest/Vitest, Supertest, MSW.
- **E2E/Visual:** Playwright (`toHaveScreenshot()`; masked regions).
- **Performance:** k6 (TS).
- **Accessibility:** axe-core, Lighthouse CI.
- **Contracts:** zod ↔ OpenAPI, Spectral lints; DB contracts (custom scripts).
- **Security:** npm audit/OSV, ZAP, Trivy/Grype, git-secrets.
- **Observability:** Prometheus scrape tests; trace smoke.
- **Coverage:** Istanbul; thresholds enforced.
- **Reporting:** Markdown templates & Node generator (optional), Codecov, GH Summary.

---

# 11. CI/CD Integration & Quality Gates

**Order (parallelized where sensible):**

1. Lint + Typecheck
2. Unit + Integration (coverage enforced)
3. Contract (OpenAPI & DB contracts)
4. E2E (Playwright + traces)
5. Accessibility (axe/Lighthouse)
6. Performance (k6) with regression guard
7. Security (SCA + ZAP baseline)
8. Observability (metrics contract)
9. Packaging: SBOM, sign (cosign), **verify signatures**, enforce **pinned digests**
10. Deploy to **staging** → release safety smoke → **UAT sign-off** → **production**

**Blocking gates:** any threshold breach, any unapproved visual diff, any contrast/token violation, SCA/ZAP high, metrics contract failure.

Artifacts are retained (≥14–30 days): coverage HTML, Playwright traces, Lighthouse/axe/k6 JSON, ZAP HTML, SBOM, signatures, metrics snapshots, visual diffs.

---

# 12. UAT & Release Management

- **UAT scripts:**
  1. Register→Verify→Login→Plan (next week)→Start/Complete (today)→Dashboard→Export CSV
  2. Privacy/i18n: deny analytics → switch to DE → reload tokens → delete account → ensure export link disabled after delete
  3. Sharing: publish a session → open link in private window → revoke → verify 404

- **Sign-off:** PO + Tech Lead + QA Lead.
- **Waivers:** documented with expiry and mitigation; link to issues/PRs.

---

# 13. Defect Management

**Severity:** S1 (critical outage/data loss) ≤24h; S2 ≤3d; S3 ≤7d; S4 next sprint.
**Workflow:** New → Triaged → In-Progress → QA Verify → Done.
**RCA:** mandatory for S1/S2; add tests/monitors; ADR if architectural.

---

# 14. Reporting & Evidence

- **Per-PR:** GH summary + Codecov delta + failing suites list.
- **Per-pipeline:** CI Test Run report (totals, suites, failures, links to artifacts).
- **Per-release:** QA Summary with gates, waivers, defects, risks, and UAT sign-off.
- **Compliance bundles:** DSR evidence; DR drill (RPO/RTO + **erasure-replay**).

_(Templates available under `/reports/templates/` if you adopt the generator script.)_

---

# 15. Backup & DR Validation

- **Quarterly restore drill** demonstrating **RPO ≤24h**, **RTO ≤4h**, and **erasure-replay** (proof that DSR delete propagates to backups ≤14 days).
- Evidence: selected backup timestamp, restore duration, data currency, replay logs, sign-off.

---

# 16. Visual Design QA (Detailed)

- **Tokens contract (VIZ-TOK-03):**
  - Assert presence of required CSS variables (`--color-*`, spacing, radii, typography scale).
  - Deny ad-hoc overrides in app code (lint rule / CI script).
- **Snapshots (VIZ-SNAP-02):**
  - Critical screens: Auth, Planner, Logger, Dashboard.
  - Themes: **light/dark**; Breakpoints: **xs, sm, md, lg**.
  - Mask timestamps/avatars/charts; seeded clock & data; diff ≤0.2%.
- **Contrast & A11y (VIZ-CNT-04 / A11Y-CORE-01):**
  - WCAG AA (body ≥4.5:1; large text ≥3:1).
- **Responsive guards (VIZ-RESP-05):**
  - No horizontal overflow; grid alignment; long-DE strings tested.
- **Reduced motion (VIZ-MOTION-09):**
  - Honors `prefers-reduced-motion`; no essential info by animation only.

**Gate:** any unapproved snapshot diff or token/contrast violation blocks merge.

---

# 17. Observability & Operability Tests (Detailed)

- **Metrics contract:**
  - HTTP histogram/counters with labels `{method, route, status_code}`
  - DB query histogram `db_query_duration_ms` (p50/p95)
  - RUM `frontend_lcp_ms` gauge
- **Trace checks:** handler spans include DB child spans; sampling documented.
- **SLOs:** alert rules for error budget consumption; dashboard snapshot archived per release.
````
